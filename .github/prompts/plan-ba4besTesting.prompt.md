# Ba4bes non-prod testing plan (Pester + fixtures)

## Recommended approach
You’ll get the best ROI with a “Pester-first + fixtures” approach: use Pester unit tests for 80–90% (by mocking HTTP/time/random/file I/O), then add a small number of integration + snapshot/contract tests that run the scripts against temp copies of README/state using recorded fixtures. Since token usage isn’t a concern, an optional fork canary (real GitHub + live 4bes) closes the last gap without touching production.

## Test matrix

| Script | Unit (Pester) | Integration (temp workspace) | Snapshot/Contract | Optional Fork Canary | Key Fixtures | Gaps/Notes |
|---|---|---|---|---|---|---|
| [Handle-PoodleInteraction.ps1](Handle-PoodleInteraction.ps1) | Mock `Invoke-RestMethod`, `Get-Content/Set-Content`, `Get-Date`; cover command detection (`!pet/!feed`), help path, 24h rolling rate-limit math, log append + cap at 100, bonus increment, “close issue only on new issue” behavior | Copy `poodle-state.json` into a temp folder; run script with params (no env reliance); assert mutated JSON fields and captured REST request payloads (via mock capture) | Contract for JSON shape: `interactions.log[]` fields, `rateLimits.<user>[]` ISO timestamps, `decay.interactionBonus` integer; snapshot reply templates (stable phrases + counts) | Run in a fork against a dedicated “Poodle Test” issue; verify exactly one comment and only expected file changes | Minimal state JSON (empty + seeded rateLimits), fixed-time timestamps, sample issue/comment texts, captured REST request bodies | Harder to isolate if there’s lots of top-level execution; dynamic properties like `$state.rateLimits.$InteractionUser` can break with odd usernames; decide/test policy for network failure (throw vs warn vs retry) |
| [Update-PoodleMood.ps1](Update-PoodleMood.ps1) | Unit-test mood computation (clamp 0–100, thresholds), reason text, decay behavior, README marker replacement; mock GraphQL (`Invoke-RestMethod`) + `Get-Date` | Temp folder with a small README containing poodle markers + state file; run script; assert only marker block changes and state fields update deterministically | Snapshot the entire `<!--START_SECTION:poodle-->…<!--END_SECTION:poodle-->` block in `README.md`; contract for the minimal GraphQL response fields you rely on | Run via `workflow_dispatch` in a fork; validate it commits `README.md` + `poodle-state.json` and is idempotent (2 runs => no diff) | Fake GraphQL response JSON, README template with markers, seeded state JSON with non-zero bonus to validate decay | GraphQL schema/permissions can drift (best caught by canary); time-sensitive logic needs frozen time to avoid flaky tests |
| [Get-RandomPost.ps1](Get-RandomPost.ps1) | Mock `Invoke-WebRequest`; cover pagination stop condition, link-filter regex, dedupe, `og:image` extraction, README replacement from `<!-- Link -->` onward; mock `Get-Random` to avoid flake | Temp README seeded with a known tail section; run with mocked web responses simulating multiple pages + a post page; assert README changed only in intended region | Contract tests around upstream HTML: post-link formats and `og:image` meta tag; snapshot generated README fragment that the script writes | Optional canary (fork, non-blocking): run periodically with real network; alert if parsing breaks | HTML fixtures for list pages + a post page, deterministic random selection, README tail fixture including `<!-- Link -->` | Most exposed to external HTML drift and network flakiness; keep canary informational rather than required |

## Suggested sequencing and minimum viable set
- Start with `Handle-PoodleInteraction.ps1` unit tests (highest branching/risk), then `Update-PoodleMood.ps1`, then `Get-RandomPost.ps1`.
- Minimum viable non-prod coverage: 3 unit suites (one per script) + 1 temp-workspace integration test per script.
- Snapshots/contract tests are the “format regression alarm” for `README.md` and `poodle-state.json`.
- Canaries should live in a fork and be non-blocking, especially for `Get-RandomPost.ps1`.
